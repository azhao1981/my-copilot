{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiz/.py/venvs/llm/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in LLMConfig has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```thinking\n",
      "Hmm, this is an interesting question about building a QA system with LangChain and Bing Search. Let me think through the key aspects...\n",
      "\n",
      "First, let's consider what we're trying to achieve here - essentially creating a system that can search the web via Bing and provide relevant answers using LLM capabilities. This combines web search, information extraction, and natural language generation.\n",
      "\n",
      "The core components we'll need:\n",
      "- LangChain as the orchestration framework\n",
      "- Bing Search API for retrieving relevant web content \n",
      "- An LLM (probably GPT or similar) to process and generate answers\n",
      "- Some way to manage the conversation flow and context\n",
      "\n",
      "Key considerations coming to mind:\n",
      "For Bing Search integration, we'll need proper API credentials and rate limiting handling. The search results will need cleaning and preprocessing before feeding to the LLM.\n",
      "\n",
      "For the LangChain implementation, we should leverage its tools and agents capabilities. The RetrievalQA chain would be perfect for this use case as it's designed for question-answering over retrieved content.\n",
      "\n",
      "Authentication and environment setup will be important - need to handle API keys securely. Should probably use environment variables.\n",
      "\n",
      "Error handling will be critical - need to gracefully handle API failures, rate limits, malformed responses etc.\n",
      "\n",
      "Let me think about the actual implementation approach...\n",
      "\n",
      "We'll want to:\n",
      "1. Set up a basic chain that takes user questions\n",
      "2. Use Bing Search tool to get relevant results \n",
      "3. Process and clean those results\n",
      "4. Feed them to an LLM to generate natural language answers\n",
      "5. Return the response to the user\n",
      "\n",
      "The LangChain components we'll need:\n",
      "- BingSearchAPIWrapper for search\n",
      "- Probably ChatOpenAI for the LLM\n",
      "- RetrievalQA chain to tie it together\n",
      "- Some text splitters and embeddings for processing\n",
      "\n",
      "Should also consider:\n",
      "- Adding memory to maintain conversation context\n",
      "- Prompt engineering for better answers\n",
      "- Result caching to improve performance\n",
      "- Input validation and sanitization\n",
      "\n",
      "Let me organize this into a clear implementation plan...\n",
      "```\n",
      "\n",
      "基于以上思考，这里是实现一个基于LangChain和Bing Search的问答系统的关键代码:\n",
      "\n",
      "```python\n",
      "import os\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.agents import load_tools\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.utilities import BingSearchAPIWrapper\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "# 设置环境变量\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
      "os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"your-bing-key\" \n",
      "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
      "\n",
      "# 初始化Bing搜索\n",
      "search = BingSearchAPIWrapper()\n",
      "\n",
      "# 初始化LLM\n",
      "llm = OpenAI(temperature=0)\n",
      "\n",
      "# 创建工具列表\n",
      "tools = load_tools([\"bing-search\"])\n",
      "\n",
      "# 初始化存储对话历史的memory\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "\n",
      "# 创建agent\n",
      "agent = initialize_agent(\n",
      "    tools,\n",
      "    llm,\n",
      "    agent=\"conversational-react-description\",\n",
      "    memory=memory,\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "# 问答函数\n",
      "def get_answer(question: str) -> str:\n",
      "    try:\n",
      "        response = agent.run(question)\n",
      "        return response\n",
      "    except Exception as e:\n",
      "        return f\"Error occurred: {str(e)}\"\n",
      "\n",
      "# 使用示例\n",
      "if __name__ == \"__main__\":\n",
      "    question = \"What is quantum computing?\"\n",
      "    answer = get_answer(question)\n",
      "    print(f\"Q: {question}\")\n",
      "    print(f\"A: {answer}\")\n",
      "```\n",
      "\n",
      "关键实现说明：\n",
      "\n",
      "1. 环境设置：\n",
      "- 需要设置OpenAI API密钥和Bing Search API密钥\n",
      "- 使用环境变量来安全存储这些敏感信息\n",
      "\n",
      "2. 主要组件：\n",
      "- BingSearchAPIWrapper: 处理与Bing搜索API的交互\n",
      "- OpenAI: 作为LLM来处理文本生成\n",
      "- ConversationBufferMemory: 存储对话历史\n",
      "- RetrievalQA: 协调搜索和问答流程\n",
      "\n",
      "3. 核心功能：\n",
      "- 使用Bing搜索获取相关信息\n",
      "- 使用LLM处理搜索结果并生成答案\n",
      "- 维护对话上下文\n",
      "- 错误处理和异常管理\n",
      "\n",
      "4. 使用方法：\n",
      "- 实例化必要的组件\n",
      "- 调用get_answer()函数提问\n",
      "- 获取生成的回答\n",
      "\n",
      "要使用这个系统，你需要：\n",
      "\n",
      "1. 安装必要的包：\n",
      "```bash\n",
      "pip install langchain openai python-dotenv\n",
      "```\n",
      "\n",
      "2. 配置环境变量：\n",
      "- OPENAI_API_KEY\n",
      "- BING_SUBSCRIPTION_KEY\n",
      "- BING_SEARCH_URL\n",
      "\n",
      "3. 可以根据需要调整以下参数：\n",
      "- LLM的temperature参数\n",
      "- 搜索结果的数量\n",
      "- 内存缓存大小\n",
      "- 错误处理策略\n",
      "\n",
      "这个实现提供了基本的问答功能，你可以根据具体需求进行扩展，比如：\n",
      "- 添加结果缓存\n",
      "- 实现更复杂的对话管理\n",
      "- 添加结果过滤和排序\n",
      "- 优化提示工程\n",
      "- 添加更多错误处理和日志记录"
     ]
    }
   ],
   "source": [
    "import my_ast\n",
    "my_ast.process_prompt('./prompt-base.md', \"\"\"\n",
    "You play the role of a software architect, AI expert, and telecommunications expert to answer questions for me.\n",
    "使用langchain 和 bingsearch 实现一个问答系统\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/tools/bing_search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Experienced programmers in any other language can pick up <b>Python</b> very quickly, and beginners find the clean syntax and indentation structure easy to learn. Whet your appetite with our <b>Python</b> 3 overview. Installer packages for <b>Python</b> on macOS downloadable from <b>python</b>.org are signed with with an Apple Developer ID Installer certificate. As of <b>Python</b> 3.11.4 and 3.12.0b1 (2023-05-23), release installer packages are signed with certificates issued to the <b>Python</b> Software Foundation (Apple Developer ID BMM5U3QVKW) ). <b>Python</b> <b>基础教程</b> Python 是一种<b>解释型</b>、面向对象、动态数据类型的高级程序设计语言。 Python 由 Guido van Rossum 于 1989 年底发明，第一个公开发行版发行于 1991 年。 像 Perl 语言一样, Python 源代码同样遵循 GPL(GNU General Public License) 协议。 <b>Python由荷兰</b>国家<b>数学</b>与计算机科学研究中心的吉多·范罗苏姆于1990年代初设计，作为一门叫做ABC语言的替代品。Python提供了高效的高级数据结构，还能简单有效地面向对象编程。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import BingSearchAPIWrapper\n",
    "\n",
    "search = BingSearchAPIWrapper(k=4)\n",
    "\n",
    "search.run(\"python\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'snippet': '量子计算是一<b>种遵循量子力学规律调控量子信息单元进行计算的新型计算模式</b>。<b>与</b>经典计算不同，量子计算遵循量子力学规律，它是能突破经典算力瓶颈的新型计算模式。量子计算机，作为执行量子计算任务的设备，以量子比特（qubit）为基本运算单元。', 'title': '<b>量子计算</b>（遵循<b>量子</b>力学规律的<b>计算</b>模式）_百度百科', 'link': 'https://baike.baidu.com/item/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/11035661'}\n",
      "{'snippet': '量子计算是利用亚原子粒子的物理学原理来执行并行计算的系统，可以在某些数学运算中更快速有效地模拟离奇的量子现象。本文介绍了量子计算的概念、作用、工作原理和现状，以及 NVIDIA cuQuantum SDK 的应用。', 'title': '科普 | <b>什么是量子计算</b>？ - 知乎', 'link': 'https://zhuanlan.zhihu.com/p/366657266'}\n",
      "{'snippet': '本文介绍了量子计算的基本概念、类型、优势和应用领域，以及全球量子计算产业的发展现状和前景。量子计算是利用量子力学的基本原理来加速解决复杂计算的过程，可以在各个行业提高效率和创新能力，但也面临着技术和市场的挑战。', 'title': '一文看懂<b>量子计算</b>：原理、应用、行业现状-36氪', 'link': 'https://www.36kr.com/p/1723178598401'}\n",
      "{'snippet': '<b>量子计算利用专门技术</b>（包括计算机硬件和利用量子力学的算法）来解决传统计算机或超级计算机无法解决或无法快速解决的复杂问题。 如今，IBM Quantum 为数十万开发人员提供了真正的量子硬件，这是一种科学家在三十年前才开始想象的工具。', 'title': '<b>什么是量子计算</b>？| IBM', 'link': 'https://www.ibm.com/cn-zh/topics/quantum-computing'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain_community.tools.bing_search import BingSearchResults\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "# 初始化 Bing 搜索\n",
    "api_wrapper = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_subscription_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "tool = BingSearchResults(api_wrapper=api_wrapper)\n",
    "question = \"什么是量子计算\"\n",
    "\n",
    "# .invoke wraps utility.results\n",
    "response = tool.invoke(question)\n",
    "response = json.loads(response.replace(\"'\", '\"'))\n",
    "for item in response:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiz/.py/venvs/llm/lib/python3.11/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m量子计算是一种基于量子力学原理的新型计算方式。传统计算机使用比特（0和1）作为基本单位，而量子计算机则使用量子比特（qubits）。量子比特可以同时处于0和1的叠加状态，这使得量子计算机在处理某些复杂问题时具有显著的优势。\n",
      "\n",
      "量子计算和量子力学的关系主要体现在以下几个方面：\n",
      "\n",
      "1. **叠加原理**：这是量子力学的一个基本原则，允许量子比特同时处于多种状态。这使得量子计算机能够同时处理大量可能的计算路径。\n",
      "\n",
      "2. **纠缠现象**：量子力学中的纠缠现象允许多个量子比特之间形成强相关性，改变一个量子比特的状态会立即影响另一个纠缠态的量子比特，即使它们相距很远。纠缠被用来实现量子计算中的高效信息传输和处理。\n",
      "\n",
      "3. **量子门操作**：量子计算使用量子门来操控量子比特状态的变化，这些操作基于量子力学的基本原理。\n",
      "\n",
      "4. **量子退相干**：量子计算中需要克服的问题之一是退相干，即量子信息与环境相互作用后丧失其量子特性。理解和解决退相干问题依赖于量子力学的深入研究。\n",
      "\n",
      "总的来说，量子计算是量子力学在计算领域的应用，利用量子力学的特性来实现传统计算机无法企及的计算能力。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '什么是量子计算?，量子计算和量子力学有什么关系？',\n",
       " 'output': '量子计算是一种基于量子力学原理的新型计算方式。传统计算机使用比特（0和1）作为基本单位，而量子计算机则使用量子比特（qubits）。量子比特可以同时处于0和1的叠加状态，这使得量子计算机在处理某些复杂问题时具有显著的优势。\\n\\n量子计算和量子力学的关系主要体现在以下几个方面：\\n\\n1. **叠加原理**：这是量子力学的一个基本原则，允许量子比特同时处于多种状态。这使得量子计算机能够同时处理大量可能的计算路径。\\n\\n2. **纠缠现象**：量子力学中的纠缠现象允许多个量子比特之间形成强相关性，改变一个量子比特的状态会立即影响另一个纠缠态的量子比特，即使它们相距很远。纠缠被用来实现量子计算中的高效信息传输和处理。\\n\\n3. **量子门操作**：量子计算使用量子门来操控量子比特状态的变化，这些操作基于量子力学的基本原理。\\n\\n4. **量子退相干**：量子计算中需要克服的问题之一是退相干，即量子信息与环境相互作用后丧失其量子特性。理解和解决退相干问题依赖于量子力学的深入研究。\\n\\n总的来说，量子计算是量子力学在计算领域的应用，利用量子力学的特性来实现传统计算机无法企及的计算能力。'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "instructions = \"\"\"You are an assistant.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key, \n",
    "    base_url=base_url, \n",
    "    model=\"gpt-4o\"\n",
    "    # model=\"claude-3-5-sonnet\"\n",
    ")\n",
    "api_wrapper = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_subscription_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "tool = BingSearchResults(api_wrapper=api_wrapper)\n",
    "tools = [tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"什么是量子计算?，量子计算和量子力学有什么关系？\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiz/.py/venvs/llm/lib/python3.11/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7f7557465580>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7f7557465580>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': [], 'instructions': 'You are an assistant.'} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'openai-functions-template', 'lc_hub_commit_hash': 'b4198088b0f11cbe911a9e1cb6546893c00283b6cf0f80ae003b3955efdfe281'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['instructions'], input_types={}, partial_variables={}, template='{instructions}'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "instructions = \"\"\"You are an assistant.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/llms/anthropic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "model = AnthropicLLM(\n",
    "    api_key=api_key, \n",
    "    base_url=base_url, \n",
    "    model=\"claude-3-5-sonnet\"\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_result': '量子计算是<b>一种遵循量子力学规律调控量子信息单元进行计算的新型计算模式</b>。与经典计算不同，量子计算遵循量子力学规律，它是能突破经典算力瓶颈的新型计算模式。量子计算机，作为执行量子计算任务的设备，以量子比特（qubit）为基本运算单元。 <b>量子计算使用亚原子粒子的物理学领域来执行复杂的并行计算</b>，<b>从而取代了当今计算机系统中更简单的晶体管</b>。 量子计算机使用量子比特计算，计算单元可以打开，关闭或之间的任何值，而不是传统计算机中的字符，要么打开，要么关闭，要么是 1，要么是 0。 量子比特居于中间态的能力（称为“态叠加”），这为计算方程增加了强大的功能，使量子计算机在某种数学运算中更胜一筹。 量子计算机的作用. 量子计算机可以通过量子比特进行计算，这种计算过程需要耗费传统计算机无限长的时间，有时甚至根本无法完成。 例如，如今的计算机使用 8 位表示介于 0 到 255 之间的任何数字。 得益于态叠加原理，量子计算机可以使用八个量子比特同时表示 0 到 255 之间的每个数字。 什么是量子计算？ 量子计算机<b>可以比传统计算机更有效地处理大量复杂的数据集</b>。 他们利用量子力学的基本原理来加速解决复杂计算的过程。 <b>量子计算</b>利用专门技术（包括计算机硬件和利用量子力学的算法）来解决传统计算机或超级计算机无法解决或无法快速解决的复杂问题。 如今，IBM Quantum 为数十万开发人员提供了真正的量子硬件，这是一种科学家在三十年前才开始想象的工具。 <b>量子计算是</b>一个<b>多学科领域</b>，涵盖<b>计算机科学</b>、<b>物理学和数学的各个</b>方面，它利用量子力学，可以比传统计算机更快地解决复杂问题。 量子计算领域包括硬件研究和应用程序开发。 本文介绍量子计算的原则、与经典计算的比较方式，以及它如何使用量子力学原理。 由于在经典计算机上模拟量子系统比较困难，由此产生了量子计算机的想法。 在 20 世纪 80 年代，Richard Feynman 和 Yuri Manin 分别提出，基于量子现象的硬件在模拟量子系统方面可能比传统计算机更有效。 可以通过多种方法来了解量子力学难以模拟的原因。 最简单的就是看物质在量子层面上有多种可能的构型（称为状态）。 假设电子系统有 40 个可能的位置，其中每个位置可以有或没有电子。 因此，系统可能处于 2^ {40} 配 置 中 的 任 何 配 置 中 的 任 何 一个（因为每个位置都有两个可能的配置，要么有电子或空）。 若要将电子的量子状态存储在传统计算机内存中，需要超过 130 GB 的内存！ <b>量子计算机能力的诀窍在于它能够制备量子比特并对其进行控制</b>。 什么是量子比特？ 一方面，现在的计算机使用比特 （bit），它是一种表示1或0的电脉冲或光脉冲流。 从微博、微信、iTunes歌曲到视频服务的所有内容都是这些二进制数字的长串。 而量子计算机则使用了量子比特，其物理载体通常是亚原子粒子，例如电子或光子。 就量子计算机而言，制备量子比特并对其进行控制是一项科学挑战。 一些如IBM、谷歌、 Rigetti Computing 、阿里巴巴、本源量子等公司，在量子计算机运行时，超导电路的冷却温度几乎接近宇宙最低温。 而其它像 IonQ 一样的公司，则在超高真空室中的硅芯片电磁场中去捕获单个原子。 通过这两种方法，能够将量子比特隔离以达到控制的目的。 量子计算是一<b>种范式变革技术</b>，<b>可利用量子物理学定律来处理极其困难的问题</b>。 量子计算利用量子物理学原理解锁了一种全新计算方式。 什么是量子计算？ “量子计算&quot;&quot;”中的量子是指<b>系统用来计算输出的量子力学</b>。 在物理学中，量子是所有物理特性的最小离散单元。 它通常指原子或亚原子粒子（如电子、中微子和光子）的属性。 什么是量子计算？ 量子计算是<b>一种进行并行计算的复杂方法</b>，<b>使用控制亚原子粒子的物理学来取代当今计算机中更简单的晶体管</b>。 量子计算机使用量子比特进行计算，计算单位可以打开、关闭或之间的任何值，而不是传统计算机中打开或关闭、1或零的位。', 'question': '什么是量子计算？', 'text': '量子计算是一种新型的计算模式，它基于量子力学的基本原理来执行计算任务。与传统的经典计算不同，量子计算利用亚原子粒子的物理特性来实现并行计算，从而能够解决一些经典计算机无法解决或需要很长时间才能解决的复杂问题。\\n\\n量子计算的核心是量子比特（qubit），这是量子计算机的基本运算单元。与经典计算机使用的比特不同，比特只能表示0或1两种状态，而量子比特可以利用量子态叠加的特性同时表示0和1的多种状态。这种能力使得量子计算机在某些数学运算中具有更强的计算能力。\\n\\n量子计算机通过制备和控制量子比特来进行运算。制备量子比特并对其进行精确控制是量子计算中的一个关键挑战。目前，许多公司，如IBM、谷歌、Rigetti Computing和阿里巴巴等，正在研究和开发量子计算机技术。\\n\\n量子计算是一个多学科领域，涵盖了计算机科学、物理学和数学的各个方面。它不仅在理论上提供了一种全新的计算方式，而且在实际应用中也展示了其潜力，特别是在处理大量复杂数据集方面。量子计算有可能带来范式变革，解决传统计算机无法快速解决的极其困难的问题。'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "# 初始化 Bing 搜索\n",
    "search = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_subscription_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    # k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "search_tool = Tool(\n",
    "    name=\"Bing Search\",\n",
    "    description=\"用于搜索互联网信息的工具\",\n",
    "    func=search.run\n",
    ")\n",
    "\n",
    "def search(question):\n",
    "    search_result = search_tool.run(question)\n",
    "    return search_result\n",
    "\n",
    "# 设置提示模板\n",
    "prompt_template = \"\"\"根据以下信息回答问题：\n",
    "\n",
    "搜索结果: {search_result}\n",
    "问题: {question}\n",
    "\n",
    "请提供详细的答案：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"search_result\", \"question\"]\n",
    ")\n",
    "\n",
    "# 创建 LLM Chain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key, \n",
    "    base_url=base_url, \n",
    "    model=\"gpt-4o\"\n",
    "    # model=\"claude-3-5-sonnet\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def get_answer(question):\n",
    "    try:\n",
    "        # 执行搜索\n",
    "        search_result = search_tool.run(question)\n",
    "        \n",
    "        # 生成答案\n",
    "        response = chain.invoke({\n",
    "            \"search_result\": search_result,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"发生错误: {str(e)}\"\n",
    "\n",
    "# 使用示例\n",
    "question = \"什么是量子计算？\"\n",
    "answer = get_answer(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "print(api_key)\n",
    "print(base_url)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "response = llm.stream(\"你是哪个版本的模型\")\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "prompt = open('prompt-base.md', 'r', encoding='utf-8').read()\n",
    "prompt = prompt + \"\"\"\n",
    "把下面的代码进行函数和类抽取重构，使之更简洁，易扩展，易维护，代码不需要注释\n",
    "新代码应该支持修改模型名称，是否使用流式\n",
    "```python\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "\n",
    "prompt = open('prompt-base.md', 'r', encoding='utf-8').read()\n",
    "prompt = prompt + \"1.8 和 1.11 谁大\"\n",
    "response = llm.stream(prompt)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "file_path = f'sessions/temp-{timestamp}.md'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for chunk in response:\n",
    "        print(chunk.content, end=\"\")\n",
    "        file.write(chunk.content)\n",
    "```\n",
    "\"\"\"\n",
    "response = llm.stream(prompt)\n",
    "# 获取当前时间戳\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# 定义文件路径\n",
    "file_path = f'sessions/temp-{timestamp}.md'\n",
    "\n",
    "# 将内容写入文件\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for chunk in response:\n",
    "        # print(chunk.content, end=\"\")\n",
    "        file.write(chunk.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
