{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiz/.py/venvs/llm/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in LLMConfig has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```thinking\n",
      "Hmm, this is an interesting system design question about building a QA system with LangChain and Bing Search... Let me think through this carefully.\n",
      "\n",
      "First, I need to break down the core components needed here. We'll need LangChain as the framework to orchestrate everything, and Bing Search API to retrieve relevant information from the web. The basic flow would be: take user question -> search Bing -> process results -> generate answer.\n",
      "\n",
      "The key elements we'll need:\n",
      "- Bing Search API credentials and setup\n",
      "- LangChain components for search, text processing, and response generation\n",
      "- A way to structure the retrieved information\n",
      "- Proper prompt engineering to get good answers\n",
      "\n",
      "I should think about potential challenges:\n",
      "- Rate limits and costs of the Bing API\n",
      "- Processing and filtering the search results effectively\n",
      "- Ensuring relevant and accurate answers\n",
      "- Error handling and edge cases\n",
      "\n",
      "For implementation approach, we'll want to:\n",
      "1. Set up the environment and dependencies\n",
      "2. Configure Bing Search integration\n",
      "3. Create the chain of components\n",
      "4. Add proper error handling\n",
      "5. Format the final responses\n",
      "\n",
      "Let me think about the specific LangChain components we'll need...\n",
      "We can use:\n",
      "- BingSearchAPIWrapper for search\n",
      "- LLMChain for processing\n",
      "- Perhaps a custom prompt template\n",
      "- Maybe some text splitters and embeddings for better context\n",
      "\n",
      "The response should include clear setup instructions and example code. I'll structure it to be easy to follow and implement.\n",
      "\n",
      "Wait, I should also consider adding some best practices and optimization tips, like:\n",
      "- Caching search results\n",
      "- Proper API key management\n",
      "- Rate limiting considerations\n",
      "- Response quality improvements\n",
      "\n",
      "Alright, I think I have a good handle on how to explain this implementation clearly...\n",
      "```\n",
      "\n",
      "让我为您详细介绍如何使用 LangChain 和 Bing Search 构建一个问答系统。以下是完整的实现步骤：\n",
      "\n",
      "1. 首先，安装必要的依赖：\n",
      "\n",
      "```python\n",
      "pip install langchain\n",
      "pip install python-dotenv\n",
      "```\n",
      "\n",
      "2. 配置环境变量：\n",
      "创建一个 .env 文件，添加必要的 API keys：\n",
      "\n",
      "```python\n",
      "BING_SEARCH_API_KEY=your_bing_api_key\n",
      "OPENAI_API_KEY=your_openai_api_key\n",
      "```\n",
      "\n",
      "3. 实现基本的问答系统：\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.utilities import BingSearchAPIWrapper\n",
      "from langchain.agents import Tool\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from dotenv import load_dotenv\n",
      "import os\n",
      "\n",
      "# 加载环境变量\n",
      "load_dotenv()\n",
      "\n",
      "# 初始化 Bing 搜索\n",
      "search = BingSearchAPIWrapper()\n",
      "\n",
      "# 创建搜索工具\n",
      "search_tool = Tool(\n",
      "    name=\"Bing Search\",\n",
      "    description=\"用于搜索互联网信息的工具\",\n",
      "    func=search.run\n",
      ")\n",
      "\n",
      "# 设置提示模板\n",
      "prompt_template = \"\"\"根据以下信息回答问题：\n",
      "\n",
      "搜索结果: {search_result}\n",
      "问题: {question}\n",
      "\n",
      "请提供详细的答案：\"\"\"\n",
      "\n",
      "prompt = PromptTemplate(\n",
      "    template=prompt_template,\n",
      "    input_variables=[\"search_result\", \"question\"]\n",
      ")\n",
      "\n",
      "# 创建 LLM Chain\n",
      "llm = OpenAI(temperature=0.7)\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "def get_answer(question):\n",
      "    try:\n",
      "        # 执行搜索\n",
      "        search_result = search_tool.run(question)\n",
      "        \n",
      "        # 生成答案\n",
      "        response = chain.run({\n",
      "            \"search_result\": search_result,\n",
      "            \"question\": question\n",
      "        })\n",
      "        \n",
      "        return response\n",
      "    except Exception as e:\n",
      "        return f\"发生错误: {str(e)}\"\n",
      "\n",
      "# 使用示例\n",
      "if __name__ == \"__main__\":\n",
      "    question = \"什么是量子计算？\"\n",
      "    answer = get_answer(question)\n",
      "    print(answer)\n",
      "```\n",
      "\n",
      "4. 优化版本 - 添加缓存和错误处理：\n",
      "\n",
      "```python\n",
      "from langchain.cache import InMemoryCache\n",
      "from langchain.globals import set_llm_cache\n",
      "import time\n",
      "\n",
      "# 设置缓存\n",
      "set_llm_cache(InMemoryCache())\n",
      "\n",
      "class QASystem:\n",
      "    def __init__(self):\n",
      "        self.search = BingSearchAPIWrapper()\n",
      "        self.llm = OpenAI(temperature=0.7)\n",
      "        self.setup_chain()\n",
      "        self.last_query_time = 0\n",
      "        self.min_query_interval = 1  # 最小查询间隔（秒）\n",
      "\n",
      "    def setup_chain(self):\n",
      "        self.search_tool = Tool(\n",
      "            name=\"Bing Search\",\n",
      "            description=\"Search tool for internet information\",\n",
      "            func=self.search.run\n",
      "        )\n",
      "        \n",
      "        prompt_template = \"\"\"根据以下信息回答问题：\n",
      "        搜索结果: {search_result}\n",
      "        问题: {question}\n",
      "        请提供详细的答案：\"\"\"\n",
      "        \n",
      "        prompt = PromptTemplate(\n",
      "            template=prompt_template,\n",
      "            input_variables=[\"search_result\", \"question\"]\n",
      "        )\n",
      "        \n",
      "        self.chain = LLMChain(llm=self.llm, prompt=prompt)\n",
      "\n",
      "    def rate_limit(self):\n",
      "        current_time = time.time()\n",
      "        if current_time - self.last_query_time < self.min_query_interval:\n",
      "            time.sleep(self.min_query_interval - (current_time - self.last_query_time))\n",
      "        self.last_query_time = time.time()\n",
      "\n",
      "    def get_answer(self, question):\n",
      "        try:\n",
      "            self.rate_limit()\n",
      "            \n",
      "            # 执行搜索\n",
      "            search_result = self.search_tool.run(question)\n",
      "            \n",
      "            # 生成答案\n",
      "            response = self.chain.run({\n",
      "                \"search_result\": search_result,\n",
      "                \"question\": question\n",
      "            })\n",
      "            \n",
      "            return {\n",
      "                \"status\": \"success\",\n",
      "                \"answer\": response,\n",
      "                \"search_result\": search_result\n",
      "            }\n",
      "            \n",
      "        except Exception as e:\n",
      "            return {\n",
      "                \"status\": \"error\",\n",
      "                \"message\": str(e)\n",
      "            }\n",
      "\n",
      "# 使用示例\n",
      "qa_system = QASystem()\n",
      "result = qa_system.get_answer(\"解释什么是机器学习？\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "主要功能和优化点：\n",
      "\n",
      "1. 基本功能：\n",
      "- 集成了 Bing 搜索功能\n",
      "- 使用 LangChain 处理搜索结果\n",
      "- 通过 LLM 生成最终答案\n",
      "\n",
      "2. 优化特性：\n",
      "- 添加了内存缓存来避免重复查询\n",
      "- 实现了简单的速率限制\n",
      "- 完善的错误处理机制\n",
      "- 结构化的响应格式\n",
      "\n",
      "3. 使用建议：\n",
      "- 确保正确设置所有必要的 API 密钥\n",
      "- 根据需求调整 temperature 参数\n",
      "- 可以根据具体需求修改提示模板\n",
      "- 考虑添加结果持久化存储\n",
      "- 可以添加更多的错误重试机制\n",
      "\n",
      "4. 可能的扩展：\n",
      "- 添加数据库存储历史查询\n",
      "- 实现更复杂的提示工程\n",
      "- 添加结果过滤和排序机制\n",
      "- 实现并行搜索和处理\n",
      "- 添加用户反馈机制\n",
      "\n",
      "使用这个系统时，请确保：\n",
      "1. 已经获取了必要的 API 密钥\n",
      "2. 正确设置了环境变量\n",
      "3. 安装了所有必要的依赖\n",
      "4. 考虑了 API 调用限制和成本\n",
      "\n",
      "这个实现提供了一个良好的起点，您可以根据具体需求进行进一步的定制和优化。"
     ]
    }
   ],
   "source": [
    "import my_ast\n",
    "my_ast.process_prompt('./prompt-base.md', \"\"\"\n",
    "You play the role of a software architect, AI expert, and telecommunications expert to answer questions for me.\n",
    "使用langchain 和 bingsearch 实现一个问答系统\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Experienced programmers in any other language can pick up <b>Python</b> very quickly, and beginners find the clean syntax and indentation structure easy to learn. Whet your appetite with our <b>Python</b> 3 overview. Installer packages for <b>Python</b> on macOS downloadable from <b>python</b>.org are signed with with an Apple Developer ID Installer certificate. As of <b>Python</b> 3.11.4 and 3.12.0b1 (2023-05-23), release installer packages are signed with certificates issued to the <b>Python</b> Software Foundation (Apple Developer ID BMM5U3QVKW) ). <b>Python</b> <b>基础教程</b> Python 是一种<b>解释型</b>、面向对象、动态数据类型的高级程序设计语言。 Python 由 Guido van Rossum 于 1989 年底发明，第一个公开发行版发行于 1991 年。 像 Perl 语言一样, Python 源代码同样遵循 GPL(GNU General Public License) 协议。 使用 <b>Python</b> <b>进行计算很</b>简单，表达式语法也很简单：运算符+, -, * 和 / 按预期工作; 括号 可用于分组. 有关 Python 3 中简单数学函数的更多信息 . # 列表上的 For 循环 &gt;&gt;&gt; numbers = [2, 4, 6, 8] &gt;&gt;&gt; product = 1 &gt;&gt;&gt; for number in numbers: ...'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import BingSearchAPIWrapper\n",
    "\n",
    "search = BingSearchAPIWrapper(k=4)\n",
    "\n",
    "search.run(\"python\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BingSearchAPIWrapper.results() missing 1 required positional argument: 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m search_result\n\u001b[1;32m     25\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m什么是量子计算\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m search_result \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(search_result)\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(question):\n\u001b[0;32m---> 22\u001b[0m     search_result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m search_result\n",
      "File \u001b[0;32m~/.py/venvs/llm/lib/python3.11/site-packages/langchain_core/tools/base.py:689\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    688\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    690\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    691\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.py/venvs/llm/lib/python3.11/site-packages/langchain_core/tools/base.py:657\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    656\u001b[0m     tool_kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 657\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/.py/venvs/llm/lib/python3.11/site-packages/langchain_core/tools/simple.py:92\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     91\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: BingSearchAPIWrapper.results() missing 1 required positional argument: 'num_results'"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "# 初始化 Bing 搜索\n",
    "search = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_subscription_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "search_tool = Tool(\n",
    "    name=\"Bing Search\",\n",
    "    description=\"用于搜索互联网信息的工具\",\n",
    "    func=search.run\n",
    ")\n",
    "\n",
    "def search(question):\n",
    "    search_result = search_tool.run(question)\n",
    "    return search_result\n",
    "\n",
    "question = \"什么是量子计算\"\n",
    "search_result = search(question)\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Experienced programmers in any other language can pick up <b>Python</b> very quickly, and beginners find the clean syntax and indentation structure easy to learn. Whet your appetite with our <b>Python</b> 3 overview. Installer packages for <b>Python</b> on macOS downloadable from <b>python</b>.org are signed with with an Apple Developer ID Installer certificate. As of <b>Python</b> 3.11.4 and 3.12.0b1 (2023-05-23), release installer packages are signed with certificates issued to the <b>Python</b> Software Foundation (Apple Developer ID BMM5U3QVKW) ). <b>Python</b> <b>基础教程</b> Python 是一种<b>解释型</b>、面向对象、动态数据类型的高级程序设计语言。 Python 由 Guido van Rossum 于 1989 年底发明，第一个公开发行版发行于 1991 年。 像 Perl 语言一样, Python 源代码同样遵循 GPL(GNU General Public License) 协议。 使用 <b>Python</b> <b>进行计算很</b>简单，表达式语法也很简单：运算符+, -, * 和 / 按预期工作; 括号 可用于分组. 有关 Python 3 中简单数学函数的更多信息 . # 列表上的 For 循环 &gt;&gt;&gt; numbers = [2, 4, 6, 8] &gt;&gt;&gt; product = 1 &gt;&gt;&gt; for number in numbers: ...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的信息，我可以对量子计算做如下详细解释：\n",
      "\n",
      "量子计算是一种全新的计算模式，其核心特点如下：\n",
      "\n",
      "1. 基本定义：\n",
      "- 这是一种遵循量子力学规律调控量子信息单元进行计算的新型计算模式\n",
      "- 使用亚原子粒子的物理学领域来执行复杂的并行计算\n",
      "- 将原子、电子或光子等基本粒子的量子特性与量子比特结合起来，作为计算中的一种新形式的信息编码\n",
      "\n",
      "2. 核心特征：\n",
      "- 使用量子比特(qubit)作为基本运算单元，而不是传统计算机的晶体管\n",
      "- 量子比特可以处于叠加态，即不仅可以是0或1，还可以同时存在多个状态\n",
      "- 利用量子力学的基本原理来加速解决复杂计算的过程\n",
      "\n",
      "3. 优势：\n",
      "- 可以比传统计算机更有效地处理大量复杂的数据集\n",
      "- 能够突破经典算力瓶颈\n",
      "- 在某些特定计算任务上，效率远超传统计算机\n",
      "- 可以解决传统计算机或超级计算机无法解决或无法快速解决的复杂问题\n",
      "\n",
      "4. 应用原理：\n",
      "- 通过量子叠加和纠缠等物理特性进行计算\n",
      "- 利用专门的硬件和基于量子力学的算法\n",
      "- 可以同时处理多个状态，实现并行计算\n",
      "\n",
      "这种新型计算模式代表了计算技术的一次革命性突破，是未来计算发展的重要方向。\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 初始化 Bing 搜索\n",
    "search = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_search_api_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    # k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "search_tool = Tool(\n",
    "    name=\"Bing Search\",\n",
    "    description=\"用于搜索互联网信息的工具\",\n",
    "    func=search.run\n",
    ")\n",
    "\n",
    "def search(question):\n",
    "    search_result = search_tool.run(question)\n",
    "    return search_result\n",
    "\n",
    "# 设置提示模板\n",
    "prompt_template = \"\"\"根据以下信息回答问题：\n",
    "\n",
    "搜索结果: {search_result}\n",
    "问题: {question}\n",
    "\n",
    "请提供详细的答案：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"search_result\", \"question\"]\n",
    ")\n",
    "\n",
    "# 创建 LLM Chain\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def get_answer(question):\n",
    "    try:\n",
    "        # 执行搜索\n",
    "        search_result = search_tool.run(question)\n",
    "        \n",
    "        # 生成答案\n",
    "        response = chain.run({\n",
    "            \"search_result\": search_result,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"发生错误: {str(e)}\"\n",
    "\n",
    "# 使用示例\n",
    "question = \"什么是量子计算？\"\n",
    "answer = get_answer(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是 Claude，一个由 Anthropic 开发的 AI 助手。我会努力用专业、诚实和有帮助的方式来回答你的问题。"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "response = llm.stream(\"你是哪个版本的模型\")\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "prompt = open('prompt-base.md', 'r', encoding='utf-8').read()\n",
    "prompt = prompt + \"\"\"\n",
    "把下面的代码进行函数和类抽取重构，使之更简洁，易扩展，易维护，代码不需要注释\n",
    "新代码应该支持修改模型名称，是否使用流式\n",
    "```python\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "\n",
    "prompt = open('prompt-base.md', 'r', encoding='utf-8').read()\n",
    "prompt = prompt + \"1.8 和 1.11 谁大\"\n",
    "response = llm.stream(prompt)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "file_path = f'sessions/temp-{timestamp}.md'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for chunk in response:\n",
    "        print(chunk.content, end=\"\")\n",
    "        file.write(chunk.content)\n",
    "```\n",
    "\"\"\"\n",
    "response = llm.stream(prompt)\n",
    "# 获取当前时间戳\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# 定义文件路径\n",
    "file_path = f'sessions/temp-{timestamp}.md'\n",
    "\n",
    "# 将内容写入文件\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for chunk in response:\n",
    "        # print(chunk.content, end=\"\")\n",
    "        file.write(chunk.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
