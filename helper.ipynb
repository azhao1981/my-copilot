{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_ast\n",
    "my_ast.process_prompt('./prompt-base.md', \"\"\"\n",
    "You play the role of a software architect, AI expert, and telecommunications expert to answer questions for me.\n",
    "使用langchain 和 bingsearch 实现一个问答系统\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/tools/bing_search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import BingSearchAPIWrapper\n",
    "\n",
    "search = BingSearchAPIWrapper(k=4)\n",
    "\n",
    "search.run(\"python\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain_community.tools.bing_search import BingSearchResults\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "# 初始化 Bing 搜索\n",
    "api_wrapper = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_subscription_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "tool = BingSearchResults(api_wrapper=api_wrapper)\n",
    "question = \"什么是量子计算\"\n",
    "\n",
    "# .invoke wraps utility.results\n",
    "response = tool.invoke(question)\n",
    "response = json.loads(response.replace(\"'\", '\"'))\n",
    "for item in response:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "instructions = \"\"\"You are an assistant.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key, \n",
    "    base_url=base_url, \n",
    "    model=\"gpt-4o\"\n",
    "    # model=\"claude-3-5-sonnet\"\n",
    ")\n",
    "api_wrapper = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_subscription_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "tool = BingSearchResults(api_wrapper=api_wrapper)\n",
    "tools = [tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"What happened in the latest burning man floods?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'input': 'What happened in the latest burning man floods?',\n",
    " 'output': 'The latest Burning Man floods in 2023 caused significant disruptions. Here are some key points:\\n\\n1. **Flood Conditions**: Torrential rains turned the Black Rock Desert, where Burning Man is held, into a muddy landscape. Attendees had to contend with knee-deep mud, and there were warnings to conserve food and water. They were also ordered to shelter in place due to the conditions [NPR](https://www.npr.org/2023/09/03/1197497458/the-latest-on-the-burning-man-flooding).\\n\\n2. **Stranded Attendees**: Tens of thousands of partygoers were stranded in the mud-caked northern Nevada desert due to flooded roads, leading to long wait times for exiting the area [AP News](https://apnews.com/article/burning-man-flooding-nevada-stranded-0726190c9f8378935e2a3cce7f154785).\\n\\n3. **Rumors and Incidents**: There were rumors of Ebola and a confirmed death during the festival. However, the specifics of these incidents were not detailed in the search results [TODAY](https://www.today.com/news/what-is-burning-man-flood-death-rcna103231).\\n\\n4. **Exodus and Cleanup**: As conditions improved, attendees began to leave, and traffic jams eased. The festival concluded with a significant cleanup effort due to the muddy aftermath [AP News](https://apnews.com/article/burning-man-flooding-nevada-cleanup-cebf509a4b0bf666f443363ed89c342b).\\n\\nThese events highlight the challenges faced by participants and organizers in dealing with unexpected weather conditions at the festival.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/llms/anthropic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "model = AnthropicLLM(\n",
    "    api_key=api_key, \n",
    "    base_url=base_url, \n",
    "    model=\"claude-3-5-sonnet\"\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 初始化 Bing 搜索\n",
    "search = BingSearchAPIWrapper(\n",
    "    bing_search_url=os.getenv(\"BING_SEARCH_URL\", \"\"),\n",
    "    bing_search_api_key=os.getenv(\"BING_SEARCH_API_KEY\", \"\"),\n",
    "    # k=4\n",
    ")\n",
    "\n",
    "# 创建搜索工具\n",
    "search_tool = Tool(\n",
    "    name=\"Bing Search\",\n",
    "    description=\"用于搜索互联网信息的工具\",\n",
    "    func=search.run\n",
    ")\n",
    "\n",
    "def search(question):\n",
    "    search_result = search_tool.run(question)\n",
    "    return search_result\n",
    "\n",
    "# 设置提示模板\n",
    "prompt_template = \"\"\"根据以下信息回答问题：\n",
    "\n",
    "搜索结果: {search_result}\n",
    "问题: {question}\n",
    "\n",
    "请提供详细的答案：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"search_result\", \"question\"]\n",
    ")\n",
    "\n",
    "# 创建 LLM Chain\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def get_answer(question):\n",
    "    try:\n",
    "        # 执行搜索\n",
    "        search_result = search_tool.run(question)\n",
    "        \n",
    "        # 生成答案\n",
    "        response = chain.run({\n",
    "            \"search_result\": search_result,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"发生错误: {str(e)}\"\n",
    "\n",
    "# 使用示例\n",
    "question = \"什么是量子计算？\"\n",
    "answer = get_answer(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "response = llm.stream(\"你是哪个版本的模型\")\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "prompt = open('prompt-base.md', 'r', encoding='utf-8').read()\n",
    "prompt = prompt + \"\"\"\n",
    "把下面的代码进行函数和类抽取重构，使之更简洁，易扩展，易维护，代码不需要注释\n",
    "新代码应该支持修改模型名称，是否使用流式\n",
    "```python\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "api_key=os.getenv(\"ONEAPI_API_KEY\", \"\")\n",
    "base_url=os.getenv(\"ONEAPI_BASE_URL\", \"\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=\"claude-3-5-sonnet\")\n",
    "\n",
    "prompt = open('prompt-base.md', 'r', encoding='utf-8').read()\n",
    "prompt = prompt + \"1.8 和 1.11 谁大\"\n",
    "response = llm.stream(prompt)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "file_path = f'sessions/temp-{timestamp}.md'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for chunk in response:\n",
    "        print(chunk.content, end=\"\")\n",
    "        file.write(chunk.content)\n",
    "```\n",
    "\"\"\"\n",
    "response = llm.stream(prompt)\n",
    "# 获取当前时间戳\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# 定义文件路径\n",
    "file_path = f'sessions/temp-{timestamp}.md'\n",
    "\n",
    "# 将内容写入文件\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for chunk in response:\n",
    "        # print(chunk.content, end=\"\")\n",
    "        file.write(chunk.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
